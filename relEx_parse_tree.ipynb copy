{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = './stanford-parser-full-2015-12-09/'\n",
    "path_to_jar = base_dir + 'stanford-parser.jar'#'path_to/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = base_dir + 'stanford-parser-3.6.0-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "dep = result.next()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n",
      "\tat edu.stanford.nlp.parser.common.ParserGrammar.<clinit>(ParserGrammar.java:46)\n",
      "Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : [u'/usr/bin/java', u'-mx1000m', '-cp', '../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser.jar:../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar', u'edu.stanford.nlp.parser.lexparser.LexicalizedParser', u'-model', '../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models/edu/stanford/nlp/models/lexparser', u'-sentences', u'newline', u'-outputFormat', u'penn', u'-encoding', u'utf8', '/var/folders/8z/570bsh7n05gd5nz5lphxym3r0000gn/T/tmpZLk9IT']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec2db76db1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanford\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, My name is Melroy.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"What is your name?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;34m'-outputFormat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_OUTPUT_FORMAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         ]\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_trees_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtagged_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 stdout, stderr = java(cmd, classpath=self._classpath,\n\u001b[0;32m--> 212\u001b[0;31m                                       stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Java command failed : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Java command failed : [u'/usr/bin/java', u'-mx1000m', '-cp', '../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser.jar:../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar', u'edu.stanford.nlp.parser.lexparser.LexicalizedParser', u'-model', '../nlp_tools/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models/edu/stanford/nlp/models/lexparser', u'-sentences', u'newline', u'-outputFormat', u'penn', u'-encoding', u'utf8', '/var/folders/8z/570bsh7n05gd5nz5lphxym3r0000gn/T/tmpZLk9IT']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse import stanford\n",
    "\n",
    "base_dir = '../nlp_tools/stanford-parser-full-2015-12-09/'\n",
    "\n",
    "os.environ['STANFORD_PARSER'] = base_dir + 'stanford-parser.jar'\n",
    "os.environ['STANFORD_MODELS'] = base_dir + 'stanford-parser-3.6.0-models.jar'\n",
    "os.environ['JAVAHOME'] = \n",
    "\n",
    "#/englishPCFG.ser.gz\n",
    "model_path = base_dir + 'stanford-parser-3.6.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz'\n",
    "\n",
    "parser = stanford.StanfordParser(model_path=model_path)\n",
    "sentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\n",
    "print sentences\n",
    "\n",
    "# GUI\n",
    "for line in sentences:\n",
    "    for sentence in line:\n",
    "        sentence.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'want', u'exempt', u'hep', 'be']\n"
     ]
    }
   ],
   "source": [
    "# CODE TO EXTRACT VERBS\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "text = \"If a parent wants to exempt their child only from the MMR , Hep A and varicella vaccines because of the aborted fetal tissue , the religious exemption would be invalid in almost every state .\"\n",
    "text = text.lower()\n",
    "text = \"\".join([ch for ch in text if ch not in string.punctuation])    \n",
    "tokens = nltk.word_tokenize(text)\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] \n",
    "tags = nltk.pos_tag(tokens)\n",
    "verbs = [w for w,t in tags if t.startswith('V')]\n",
    "res = stem_tokens(verbs, stemmer)\n",
    "print res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dep:  nsubj(ate-2, I-1)\n",
      "m:  <_sre.SRE_Match object at 0x10cd66cf0>\n",
      "m.group(1), (2):  ate-2 I-1\n",
      "dep:  root(ROOT-0, ate-2)\n",
      "m:  <_sre.SRE_Match object at 0x10cd662d8>\n",
      "m.group(1), (2):  ROOT-0 ate-2\n",
      "dep:  det(pizza-4, the-3)\n",
      "m:  <_sre.SRE_Match object at 0x10cd66cf0>\n",
      "m.group(1), (2):  pizza-4 the-3\n",
      "dep:  dobj(ate-2, pizza-4)\n",
      "m:  <_sre.SRE_Match object at 0x10cd662d8>\n",
      "m.group(1), (2):  ate-2 pizza-4\n",
      "dep:  prep_with(pizza-4, forks-6)\n",
      "m:  <_sre.SRE_Match object at 0x10cd66cf0>\n",
      "m.group(1), (2):  pizza-4 forks-6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from practnlptools.tools import Annotator\n",
    "\n",
    "annotator = Annotator()\n",
    "text = \"I ate the pizza with forks\"\n",
    "\n",
    "dep_parse = annotator.getAnnotations(text, dep_parse=True)['dep_parse']\n",
    "\n",
    "dp_list = dep_parse.split('\\n')\n",
    "pattern = re.compile(r'.+?\\((.+?), (.+?)\\)')\n",
    "edges = []\n",
    "for dep in dp_list:\n",
    "    print \"dep: \", dep\n",
    "    m = pattern.search(dep)\n",
    "    print \"m: \", m\n",
    "    edges.append((m.group(1), m.group(2)))\n",
    "    print \"m.group(1), (2): \", m.group(1), m.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph(edges)  # Well that was easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(nx.shortest_path_length(graph, source='Robots-1', target='awesomeness-12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubj(ate-2, I-1)\n",
      "root(ROOT-0, ate-2)\n",
      "det(pizza-4, the-3)\n",
      "dobj(ate-2, pizza-4)\n",
      "prep_with(pizza-4, forks-6)\n"
     ]
    }
   ],
   "source": [
    "print (dep_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nsubj(ate-2, I-1)',\n",
       " 'root(ROOT-0, ate-2)',\n",
       " 'det(pizza-4, the-3)',\n",
       " 'dobj(ate-2, pizza-4)',\n",
       " 'prep_with(pizza-4, forks-6)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dep_parse', 'chunk', 'pos', 'srl', 'syntax_tree', 'verbs', 'words', 'ner']\n"
     ]
    }
   ],
   "source": [
    "text_annotated = annotator.getAnnotations(text, dep_parse=True)\n",
    "print text_annotated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj(ate-2, I-1)\\nroot(ROOT-0, ate-2)\\ndet(pizza-4, the-3)\\ndobj(ate-2, pizza-4)\\nprep_with(pizza-4, forks-6)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_annotated['dep_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ate', 'the', 'pizza', 'with', 'forks']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_annotated['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A0': 'I', 'A1': 'the pizza', 'AM-MNR': 'with forks', 'V': 'ate'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(text_annotated['srl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
