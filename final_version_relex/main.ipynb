{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%reset -f -> clears the global namespace without user confirmation.\n",
    "%reset -f\n",
    "\n",
    "#from init import *\n",
    "#from main_functions import *\n",
    "#from utility_functions import *\n",
    "#%load main_functions.py\n",
    "%run init.py\n",
    "%run main_functions.py\n",
    "%run utility_functions.py\n",
    "\n",
    "'''\n",
    "CONFIGURATION SETUP\n",
    "'''\n",
    "SEPARATE_SENT = True\n",
    "SHOW_DP_PLOTS = True\n",
    "SHOW_REL_EXTRACTIONS = True\n",
    "NODE_SELECTION = True\n",
    "MAX_ITERATION = 100 # -1 -> to try all\n",
    "SAVE_GEFX = True\n",
    "SAVE_PAIRWISE_RELS = True\n",
    "SAVE_ALL_RELS = True\n",
    "SET_INOUT_FNAMES_IN_CODE_ARGS = False\n",
    "\n",
    "\n",
    "'''\n",
    "INPUT/OUTPUT SETUP - PASS IT AS ARGUMENTS - PARAMETERS\n",
    "'''\n",
    "data_dir = \"../../data/\"\n",
    "DATA_SET = \"mothering\"\n",
    "texts = []\n",
    "#output_dir = \"\"\n",
    "\n",
    "if SET_INOUT_FNAMES_IN_CODE_ARGS:\n",
    "    file_input = str(sys.argv[1])\n",
    "    output_dir = str(sys.argv[2])\n",
    "    file_input_name = os.path.basename(file_input)\n",
    "    file_input_name = str(file_input_name.split(\".\")[0])    \n",
    "else:\n",
    "    if DATA_SET == \"twitter\":\n",
    "        based_dir = data_dir + 'Tweets/'\n",
    "        output_dir = based_dir  + 'output/'      \n",
    "        file_input_name = 'sample.csv'\n",
    "        file_input = based_dir + file_input_name      \n",
    "        df = read_data(file_input,\"twitter\",\",\")#read the input sentences\n",
    "        texts = df['main_tweet'].tolist()\n",
    "    elif DATA_SET == \"mothering\":\n",
    "        based_dir = data_dir + 'Vaccination/'\n",
    "        output_dir = based_dir + 'output/'\n",
    "        file_input_name = 'sents.txt'\n",
    "        #file_input_name = 'sent_cdb_child_exemption.txt'\n",
    "        file_input = based_dir + file_input_name\n",
    "        df = read_data(file_input,\"mothering\",\"\\n\")#read the input sentences\n",
    "        texts = df['text'].tolist()\n",
    "    elif DATA_SET == \"test\":\n",
    "        based_dir = data_dir + 'Vaccination/'\n",
    "        output_dir = based_dir + 'output/'\n",
    "        file_input_name = \"religousInst_adverseEffects.csv\"\n",
    "        df = read_data(data_dir + \"Vaccination/mothering_contexts/\" + file_input_name ,\"mothering\",\"\\n\")\n",
    "        texts = df['sentence'].tolist()\n",
    "        print len(df)\n",
    "        \n",
    "\n",
    "f_rel = open(output_dir + file_input_name + \"_\" + \"relations_\" + str(MAX_ITERATION) + \".csv\" , \"w\")\n",
    "header = ['original_text', 'sentence','arg1','rel','arg2','type','pattern','arg1_with_pos','rel_with_pos','arg2_with_pos','arg1_prepositions', 'rel_prepositions', 'arg2_prepositions']\n",
    "dict_writer = csv.DictWriter(f_rel, header)\n",
    "dict_writer.writeheader()\n",
    "\n",
    "'''\n",
    "A few sample test cases:\n",
    "#texts = [\"the Church told all Catholic parents not to let their child get the MMR.\"]\n",
    "#texts = [\"Why Samsung Pay could gain an early lead in mobile payments.\"]\n",
    "#texts = [\"You would keep your child 's shot records at home and NOT submit that to the school...only your exemption from all shots .\"]\n",
    "#texts = [\"Parents may use their philosophical beliefs exemption for ANY vaccine they choose to do so ; you may selectively vaccinate your child and exempt them out of other vaccines ; you may also exempt out of any and all vaccines and use your exemption that way , as well .\"]\n",
    "#texts = [\"Here is the Hawaii immunization brochure , which states the exemption forms can also be obtained from the school : Immunization and TB code : Surprisingly , I do n't see anything about religiously exempting a child from the TB screening requirement in the code .\"]\n",
    "#texts.insert(0,\"I like this product and I also like the other product.\") ! Strange dep res -> prep_like\n",
    "#texts.insert(0,\"A medical exemption is out of the question - you 'd have to first find a doctor willing to exempt your child from all shots - not going to happen - and then have the medical exemption renewed every year . \") -> tokenizer failure\n",
    "#texts.insert(0,\"A state exemption is only to exempt a child from state requirements ; while in the states , the only time an exemption would come up is when using DODs schools or daycare in the states--the OP is going to Japan . \")\n",
    "#texts.insert(0,\"Even if the Church told all Catholic parents not to let their child get the MMR for instance , most parents would have to still be required to submit a religous exemption which would exempt all vaccines .\")\n",
    "'''\n",
    "texts.insert(0,\"parents get medical exemption from vaccination for their child.\")\n",
    "texts.insert(0,\"Parents get medical exemption from vaccination for their child.\")\n",
    "texts.insert(0,\"Schools don't accept children with no medical exemptions.\")\n",
    "\n",
    "annotator = Annotator()\n",
    "# string versions of all relations - used in aggregation matching\n",
    "all_rels_str = []\n",
    "# dataframe of relations\n",
    "all_rels = []\n",
    "# the final output kept in csv format. Includes, original post, sentence, and their extractions/tags.\n",
    "output = []\n",
    "\n",
    "# To keep track of running time \n",
    "start_time = time.time()\n",
    "\n",
    "for ind, t_orig in enumerate(texts):\n",
    "    if MAX_ITERATION >= 0:\n",
    "        if ind > MAX_ITERATION:\n",
    "            break\n",
    "    t_sentences = []\n",
    "    \n",
    "    try:\n",
    "        # replacing - with space -> since most of the time tokenazer fails with the presence of -\n",
    "        t_orig = t_orig.replace(\"-\",\".\")\n",
    "        #t_orig = t_orig.replace(\" don't \", \" do not \")\n",
    "        t_orig = change_nt_to_not(t_orig)\n",
    "        # tokenizing the sentence\n",
    "        t_sentences = sent_tokenize(t_orig)\n",
    "    except:\n",
    "        print \"Error in sentence tokenizer! - \", t_orig\n",
    "        \n",
    "    \n",
    "    for t in t_sentences:\n",
    "        try:\n",
    "            t_annotated = annotator.getAnnotations(t, dep_parse=True)\n",
    "        except:\n",
    "            print \"Error in sentence annotation\"\n",
    "            continue\n",
    "        try:\n",
    "            g_dir = create_dep_graph(t_annotated)\n",
    "            if g_dir is None:\n",
    "                print \"No extraction found\"\n",
    "                continue\n",
    "            if SHOW_DP_PLOTS:\n",
    "                plot_dep(g_dir,t)\n",
    "            g_undir = g_dir.to_undirected()\n",
    "        except:\n",
    "            print \"Unexpected error while extracting relations:\", sys.exc_info()[0]\n",
    "            continue\n",
    "        rels_pure, rels_simp = get_relations(g_dir, t_annotated, option=\"SVO\")\n",
    "        rels = rels_pure#rels_simp\n",
    "        if SHOW_REL_EXTRACTIONS:\n",
    "            print ind, t, \"\\n\"\n",
    "            print \"Simplifided Version:\"\n",
    "            print_relations(rels)\n",
    "            print \"More detailed Version:\"\n",
    "            print_relations(rels_pure)\n",
    "        else:\n",
    "            print ind,\n",
    "        all_rels_str = all_rels_str + get_rels_str(rels) #For simply counting the exact strings\n",
    "        all_rels = all_rels + rels # to later create a dataframe\n",
    "        for r in rels:\n",
    "            output_row = defaultdict(list)\n",
    "            output_row = r.copy()\n",
    "            output_row[\"original_text\"] = t_orig\n",
    "            output_row[\"sentence\"] = t\n",
    "            output.append(output_row)\n",
    "            #print \" output is : \", output\n",
    "            #output_subset = dict((k,output[k]) for k in header)\n",
    "            dict_writer.writerow(output_row)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print \"Relation Extraction Time: \", end_time-start_time , \"(seconds) - \", (end_time-start_time)/60, \"(min)\"\n",
    "print \"***************STATISTICS***************\"\n",
    "print \"Total number of input records (posts): \", len(texts)\n",
    "print \"Total number of extracted relations: \", len(all_rels_str)\n",
    "print_top_relations(all_rels_str,top_num=-1) \n",
    "\n",
    "df_rels = pd.DataFrame(all_rels)\n",
    "df_output = pd.DataFrame(output)\n",
    "print df_output\n",
    "\n",
    "if SAVE_ALL_RELS:\n",
    "    columns = ['original_text', 'sentence','arg1','rel','arg2','type','pattern','arg1_with_pos','rel_with_pos','arg2_with_pos', 'arg1_prepositions', 'rel_prepositions', 'arg2_prepositions']\n",
    "    df_output.to_csv(output_dir + file_input_name + \"_\" + \"output_relations_test.csv\",sep=',', encoding='utf-8',header=True, columns=columns)\n",
    "    #save_df_rels(df_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'''\n",
    "if NODE_SELECTION:\n",
    "    # get the list of different versions of an entity. Example : parents,parent,i,we -> parents\n",
    "    entity_versions = get_entity_versions(DATA_SET)    \n",
    "    df_simp = get_simp_df(df_rels.copy(),entity_versions)  \n",
    "    selected_nodes = entity_versions.keys()\n",
    "    df_rels_selected = filter_nodes(df_simp.copy(),source='arg1',target='arg2',selected_nodes = selected_nodes)\n",
    "    g_arg = create_argument_multiGraph(df_rels_selected.copy(),source='arg1',target='arg2',edge_attr = 'rel')\n",
    "    if SAVE_GEFX:\n",
    "        nx.write_gexf(g_arg, output_dir + file_input_name + \"_\" + \"g_arg_selected_\"+str(MAX_ITERATION)+\"_\"+str(time.time())+\".gexf\")\n",
    "    plot_argument_graph(g_arg)\n",
    "    if SAVE_PAIRWISE_RELS:\n",
    "        file_loc = output_dir + file_input_name + \"_\" + \"pairwise_rels_selected_\"+str(MAX_ITERATION)+\"_\"+DATA_SET+\".txt\"\n",
    "        save_pairwise_rels(file_loc,g_arg,print_option=True)      \n",
    "\n",
    "g_arg = create_argument_multiGraph(df_rels.copy(),source='arg1',target='arg2',edge_attr = 'rel')\n",
    "if SAVE_GEFX:\n",
    "    nx.write_gexf(g_arg, output_dir + file_input_name + \"_\" + \"g_arg_\"+str(MAX_ITERATION)+\"_\"+str(time.time())+\".gexf\")\n",
    "plot_argument_graph(g_arg)\n",
    "if SAVE_PAIRWISE_RELS:\n",
    "    file_loc = output_dir + file_input_name + \"_\"  + \"pairwise_rels_\"+str(MAX_ITERATION)+\"_\"+DATA_SET+\".txt\"\n",
    "    save_pairwise_rels(file_loc,g_arg,print_option=True)    \n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    main(sys.argv[1:])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'''\n",
    "# In[68]:\n",
    "\n",
    "entity = \"government\"\n",
    "df_simp[np.logical_and(df_simp['arg1']==entity,df_simp['arg2']==entity)]\n",
    "\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "selected_nodes = entity_versions.keys()\n",
    "df_rels_selected = filter_nodes(df_simp.copy(),source='arg1',target='arg2',selected_nodes = selected_nodes)\n",
    "g_arg = create_argument_multiGraph(df_rels_selected.copy(),source='arg1',target='arg2',edge_attr = 'rel')\n",
    "if SAVE_GEFX:\n",
    "    nx.write_gexf(g_arg, output_dir +  \"/gephi_data/g_arg_selected_2_\"+str(MAX_ITERATION)+\"_\"+str(time.time())+\".gexf\")\n",
    "plot_argument_graph(g_arg)\n",
    "if SAVE_PAIRWISE_RELS:\n",
    "    file_loc = output_dir + \"/pairwise_rels_selected_2_\"+str(MAX_ITERATION)+\"_\"+DATA_SET+\".txt\"\n",
    "    save_pairwise_rels(file_loc,g_arg,print_option=True)  \n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "df_rels_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "based_dir = data_dir + 'Vaccination/'\n",
    "output_dir = based_dir + 'output/'\n",
    "file_input_name = 'allsentrels.csv'\n",
    "file_input = based_dir + file_input_name\n",
    "df2 = pd.read_csv(file_input,delimiter=',',header=0,error_bad_lines=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_versions = get_entity_versions(DATA_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5e4739049d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_simp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_simp_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_versions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mselected_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_versions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#selected_nodes = ['medical prof', 'vaccines']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf2_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_simp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arg1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arg2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/behnam/Desktop/Behnam_Files/vwani_text_mining/RE_Behnam/code/final_version_relex/main_functions.py\u001b[0m in \u001b[0;36mget_simp_df\u001b[0;34m(df, entity_versions)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_simp_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_versions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;31m# lower case the letters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0marg1_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arg1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_versions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/behnam/Desktop/Behnam_Files/vwani_text_mining/RE_Behnam/code/final_version_relex/main_functions.py\u001b[0m in \u001b[0;36mglob_version\u001b[0;34m(entity, entity_versions)\u001b[0m\n\u001b[1;32m    395\u001b[0m     '''\n\u001b[1;32m    396\u001b[0m     \u001b[0mentity_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mentity_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m     \u001b[0mentity_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mentity_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\{(.*)\\}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "df_simp = get_simp_df(df2.copy(),entity_versions)  \n",
    "selected_nodes = entity_versions.keys()\n",
    "#selected_nodes = ['medical prof', 'vaccines']\n",
    "df2_selected = filter_nodes(df_simp.copy(),source='arg1',target='arg2',selected_nodes = selected_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VPDs',\n",
       " 'medical prof',\n",
       " 'government',\n",
       " 'religous inst',\n",
       " 'vaccines',\n",
       " 'parents',\n",
       " 'adverse effects',\n",
       " 'schools',\n",
       " 'children',\n",
       " 'exemptions']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_versions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e6710625f806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_argument_multiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arg1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arg2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/behnam/Desktop/Behnam_Files/vwani_text_mining/RE_Behnam/code/final_version_relex/main_functions.py\u001b[0m in \u001b[0;36mcreate_argument_multiGraph\u001b[0;34m(df, source, target, edge_attr)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;34m''' Get dataframe in which n is the source'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mdf_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 raise TypeError('Could not compare %s type with Series'\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    646\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_arg = create_argument_multiGraph(df2.copy(),source='arg1',target='arg2',edge_attr = 'rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SAVE_GEFX:\n",
    "    nx.write_gexf(g_arg, output_dir + file_input_name + \"_\" + \"g_arg_all2_\"+str(MAX_ITERATION)+\"_\"+str(time.time())+\".gexf\")\n",
    "plot_argument_graph(g_arg)\n",
    "if SAVE_PAIRWISE_RELS:\n",
    "    file_loc = output_dir + file_input_name + \"_\" + \"pairwise_rels_all2_\"+str(MAX_ITERATION)+\"_\"+DATA_SET+\".txt\"\n",
    "    save_pairwise_rels(file_loc,g_arg,print_option=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>arg1</th>\n",
       "      <th>rel</th>\n",
       "      <th>arg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28158</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>last {visit}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28159</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28160</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>{she}</td>\n",
       "      <td>{doesn}</td>\n",
       "      <td>{'t}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>last {visit}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>After our last visit I was telling him how ups...</td>\n",
       "      <td>{she}</td>\n",
       "      <td>{doesn}</td>\n",
       "      <td>{'t}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32552</th>\n",
       "      <td>I was telling the dr how she had the DTaP that...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>the {dr}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33074</th>\n",
       "      <td>I was telling the dr how she had the DTaP that...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>the {dr}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70969</th>\n",
       "      <td>I was telling him that I didn't think she woul...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71575</th>\n",
       "      <td>I was telling him that I didn't think she woul...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{him}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99905</th>\n",
       "      <td>I was telling her I didn't want 3 vaccines at ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{her}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99906</th>\n",
       "      <td>I was telling her I didn't want 3 vaccines at ...</td>\n",
       "      <td>{'t}</td>\n",
       "      <td>{want} at</td>\n",
       "      <td>3 {vaccines}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101118</th>\n",
       "      <td>I was telling her I didn't want 3 vaccines at ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{her}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101119</th>\n",
       "      <td>I was telling her I didn't want 3 vaccines at ...</td>\n",
       "      <td>{'t}</td>\n",
       "      <td>{want} at</td>\n",
       "      <td>3 {vaccines}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158540</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{dh}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158541</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>where {felt}</td>\n",
       "      <td>{tetanus}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158542</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>will {do}</td>\n",
       "      <td>the {TIG}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158833</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{dh}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158834</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>where {felt}</td>\n",
       "      <td>{tetanus}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158835</th>\n",
       "      <td>I was telling dh that if I waited till he was ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>will {do}</td>\n",
       "      <td>the {TIG}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164991</th>\n",
       "      <td>I was telling DH about the article this mornin...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{DH}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164992</th>\n",
       "      <td>I was telling DH about the article this mornin...</td>\n",
       "      <td>{it}</td>\n",
       "      <td>{thought}</td>\n",
       "      <td>{what}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165388</th>\n",
       "      <td>I was telling DH about the article this mornin...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>was {telling}</td>\n",
       "      <td>{DH}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165389</th>\n",
       "      <td>I was telling DH about the article this mornin...</td>\n",
       "      <td>{it}</td>\n",
       "      <td>{thought}</td>\n",
       "      <td>{what}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196826</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>more {people}</td>\n",
       "      <td>how will be {getting} now</td>\n",
       "      <td>chicken {pox}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196827</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{she}</td>\n",
       "      <td>{told}</td>\n",
       "      <td>{me}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196828</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{we}</td>\n",
       "      <td>would actually be {seeing}</td>\n",
       "      <td>LESS {shingles}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196829</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>{didn}</td>\n",
       "      <td>{'t}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197133</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>more {people}</td>\n",
       "      <td>how will be {getting} now</td>\n",
       "      <td>chicken {pox}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197134</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{she}</td>\n",
       "      <td>{told}</td>\n",
       "      <td>{me}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197135</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{we}</td>\n",
       "      <td>would actually be {seeing}</td>\n",
       "      <td>LESS {shingles}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197136</th>\n",
       "      <td>So we ended up talking vaccines and these are ...</td>\n",
       "      <td>{I}</td>\n",
       "      <td>{didn}</td>\n",
       "      <td>{'t}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence           arg1  \\\n",
       "28158   After our last visit I was telling him how ups...   last {visit}   \n",
       "28159   After our last visit I was telling him how ups...            {I}   \n",
       "28160   After our last visit I was telling him how ups...          {she}   \n",
       "28614   After our last visit I was telling him how ups...   last {visit}   \n",
       "28615   After our last visit I was telling him how ups...            {I}   \n",
       "28616   After our last visit I was telling him how ups...          {she}   \n",
       "32552   I was telling the dr how she had the DTaP that...            {I}   \n",
       "33074   I was telling the dr how she had the DTaP that...            {I}   \n",
       "70969   I was telling him that I didn't think she woul...            {I}   \n",
       "71575   I was telling him that I didn't think she woul...            {I}   \n",
       "99905   I was telling her I didn't want 3 vaccines at ...            {I}   \n",
       "99906   I was telling her I didn't want 3 vaccines at ...           {'t}   \n",
       "101118  I was telling her I didn't want 3 vaccines at ...            {I}   \n",
       "101119  I was telling her I didn't want 3 vaccines at ...           {'t}   \n",
       "158540  I was telling dh that if I waited till he was ...            {I}   \n",
       "158541  I was telling dh that if I waited till he was ...            {I}   \n",
       "158542  I was telling dh that if I waited till he was ...            {I}   \n",
       "158833  I was telling dh that if I waited till he was ...            {I}   \n",
       "158834  I was telling dh that if I waited till he was ...            {I}   \n",
       "158835  I was telling dh that if I waited till he was ...            {I}   \n",
       "164991  I was telling DH about the article this mornin...            {I}   \n",
       "164992  I was telling DH about the article this mornin...           {it}   \n",
       "165388  I was telling DH about the article this mornin...            {I}   \n",
       "165389  I was telling DH about the article this mornin...           {it}   \n",
       "196826  So we ended up talking vaccines and these are ...  more {people}   \n",
       "196827  So we ended up talking vaccines and these are ...          {she}   \n",
       "196828  So we ended up talking vaccines and these are ...           {we}   \n",
       "196829  So we ended up talking vaccines and these are ...            {I}   \n",
       "197133  So we ended up talking vaccines and these are ...  more {people}   \n",
       "197134  So we ended up talking vaccines and these are ...          {she}   \n",
       "197135  So we ended up talking vaccines and these are ...           {we}   \n",
       "197136  So we ended up talking vaccines and these are ...            {I}   \n",
       "\n",
       "                               rel             arg2  \n",
       "28158                was {telling}            {him}  \n",
       "28159                was {telling}            {him}  \n",
       "28160                      {doesn}             {'t}  \n",
       "28614                was {telling}            {him}  \n",
       "28615                was {telling}            {him}  \n",
       "28616                      {doesn}             {'t}  \n",
       "32552                was {telling}         the {dr}  \n",
       "33074                was {telling}         the {dr}  \n",
       "70969                was {telling}            {him}  \n",
       "71575                was {telling}            {him}  \n",
       "99905                was {telling}            {her}  \n",
       "99906                    {want} at     3 {vaccines}  \n",
       "101118               was {telling}            {her}  \n",
       "101119                   {want} at     3 {vaccines}  \n",
       "158540               was {telling}             {dh}  \n",
       "158541                where {felt}        {tetanus}  \n",
       "158542                   will {do}        the {TIG}  \n",
       "158833               was {telling}             {dh}  \n",
       "158834                where {felt}        {tetanus}  \n",
       "158835                   will {do}        the {TIG}  \n",
       "164991               was {telling}             {DH}  \n",
       "164992                   {thought}           {what}  \n",
       "165388               was {telling}             {DH}  \n",
       "165389                   {thought}           {what}  \n",
       "196826   how will be {getting} now    chicken {pox}  \n",
       "196827                      {told}             {me}  \n",
       "196828  would actually be {seeing}  LESS {shingles}  \n",
       "196829                      {didn}             {'t}  \n",
       "197133   how will be {getting} now    chicken {pox}  \n",
       "197134                      {told}             {me}  \n",
       "197135  would actually be {seeing}  LESS {shingles}  \n",
       "197136                      {didn}             {'t}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2[df2[\"sentence\"].str.contains(\"I was telling\")]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e78ba26fc1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1757\u001b[0m                                  'backfill or nearest lookups')\n\u001b[1;32m   1758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 44759: expected 1 fields, saw 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "based_dir = data_dir + 'Vaccination/'\n",
    "output_dir = based_dir + 'output/'\n",
    "#file_input_name = 'sents.txt'\n",
    "#file_input_name = 'sent_cdb_child_exemption.txt'\n",
    "file_input = based_dir + file_input_name\n",
    "df = read_data(file_input,\"mothering\",\"\\n\")#read the input sentences\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26837\n"
     ]
    }
   ],
   "source": [
    "print len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method getDependency in module practnlptools.tools:\n",
      "\n",
      "getDependency(self, parse) method of practnlptools.tools.Annotator instance\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sent = \"parents get medical exemption from vaccination for their child.\"\n",
    "print help(annotator.getDependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
