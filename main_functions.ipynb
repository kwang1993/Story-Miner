{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_node_id(word, annotation):\n",
    "    if word == \"ROOT\":\n",
    "        return \"ROOT-NNP-0\"\n",
    "    w_ind = annotation['words'].index(word)\n",
    "    return word+\"-\"+annotation['pos'][w_ind][1]+\"-\"+str(w_ind+1) \n",
    "\n",
    "def word_id_get_index(word_id):\n",
    "    return int(word_id.split('-')[2])\n",
    "\n",
    "def word_id_get_pos(word_id):\n",
    "    # word_id syntax : (NAME-POS-INDEX ex. test-nnp-3)\n",
    "    return word_id.split('-')[1]\n",
    "\n",
    "def word_id_get_word(word_id):\n",
    "    return word_id.split('-')[0]\n",
    "\n",
    "def sort_word_ids(word_ids, head_word_ind):\n",
    "    word_sorted_str = \"\"\n",
    "    word_with_pos_sorted_str = \"\"\n",
    "    \n",
    "    word_ids_tuple = map(lambda x: (word_id_get_index(x),x), word_ids)\n",
    "    word_ids_tuple.sort(key = lambda x: x[0]) #sort based on the first element - which is word's index\n",
    "    for w in word_ids_tuple:\n",
    "        if int(w[0]) == int(head_word_ind):\n",
    "            word_sorted_str += \"{\" + word_id_get_word(w[1]) + \"}\" + \" \"\n",
    "            word_with_pos_sorted_str += w[1] + \" \"            \n",
    "        else:\n",
    "            word_sorted_str += word_id_get_word(w[1]) + \" \"\n",
    "            word_with_pos_sorted_str += w[1] + \" \"\n",
    "        \n",
    "    # removing the extra space at the end\n",
    "    return word_sorted_str.strip(), word_with_pos_sorted_str.strip()\n",
    "\n",
    "def expand_rel(rel, g_dep, annotation):\n",
    "    '''\n",
    "    Expands arguments by adding extra related words, such as nn, amod, and so on. \n",
    "    Expands relations by adding extra related words, such as adverbs, and so on.\n",
    "    Returns the expanded relation.\n",
    "    Input: \n",
    "    '''\n",
    "    arg1 = rel['arg1']\n",
    "    arg2 = rel['arg2']\n",
    "    r = rel['rel']\n",
    "    \n",
    "    arg_expand_list = ['nn', 'amod', 'det', 'neg', 'prep_of', 'num', 'quantmod']\n",
    "    arg_expand_non_nnp_list = ['infmod', 'partmod', 'ref', 'prepc_of'] #'rcmod' -> it's alwasys connected to nsubj\n",
    "    rel_expand_list = ['advmod', 'mod', 'aux', 'auxpass', 'cop', 'prt','neg']\n",
    "    rel_expand_non_SVO_list = ['dobj', 'iobj']\n",
    "    \n",
    "    arguments_names = ['arg1', 'arg2']\n",
    "    '''\n",
    "    EXPAND ARGUMENTS\n",
    "    '''\n",
    "    arg_extra_ids = defaultdict(list)\n",
    "    arg_head_ind = defaultdict(list)\n",
    "    for ind, arg_name in enumerate(arguments_names):\n",
    "        arg = rel[arg_name]\n",
    "        # add current argument to the extended version\n",
    "        arg_extra_ids[arg_name].append(arg)\n",
    "        arg_head_ind[arg_name] = word_id_get_index(arg)\n",
    "        v_arg_id = arg\n",
    "        # if it is not a proper noun -> expand more to cover rcmod, infmod, and so on.        \n",
    "        if word_id_get_pos(v_arg_id) != \"NNP\": \n",
    "            arg_expand_list_final = arg_expand_list + arg_expand_non_nnp_list\n",
    "        else:\n",
    "            arg_expand_list_final = arg_expand_list\n",
    "        try:\n",
    "            g_dir_v = g_dir[v_arg_id]\n",
    "        except:\n",
    "            print \"Faild to get adjacency network of \", v_arg_id, \" while expanding it.\"\n",
    "        for word_id, e in g_dir_v.iteritems():\n",
    "            if e[\"rel\"] in arg_expand_list_final:\n",
    "                arg_extra_ids[arg_name].append(word_id) \n",
    "    \n",
    "    '''\n",
    "    EXPAND THE RELATIONSHIP\n",
    "    '''\n",
    "    rel_extra_ids = []\n",
    "    v_rel_id = r\n",
    "    rel_extra_ids.append(v_rel_id)\n",
    "    rel_head_ind = word_id_get_index(v_rel_id)\n",
    "    if rel[\"type\"] != \"SVO\":\n",
    "        rel_expand_list_final = rel_expand_list + rel_expand_non_SVO_list\n",
    "    else:\n",
    "        rel_expand_list_final = rel_expand_list\n",
    "    try:\n",
    "        g_dir_v = g_dir[v_rel_id]\n",
    "    except:\n",
    "        print \"Faild to get adjacency network of \", v_rel_id, \" while expanding it.\"\n",
    "    for word_id, e in g_dir_v.iteritems():\n",
    "        if e[\"rel\"] in rel_expand_list_final:\n",
    "            rel_extra_ids.append(word_id)\n",
    "    \n",
    "    arg1_final_word_str, arg1_final_with_pos_str = sort_word_ids(arg_extra_ids[arguments_names[0]], arg_head_ind[arguments_names[0]])\n",
    "    arg2_final_word_str, arg2_final_with_pos_str = sort_word_ids(arg_extra_ids[arguments_names[1]], arg_head_ind[arguments_names[1]])\n",
    "    rel_final_word_str, rel_final_with_pos_str = sort_word_ids(rel_extra_ids, rel_head_ind)\n",
    "    \n",
    "    \n",
    "    rel_expanded = rel\n",
    "    rel_expanded['arg1'] = arg1_final_word_str\n",
    "    rel_expanded['arg2'] = arg2_final_word_str\n",
    "    rel_expanded['rel'] = rel_final_word_str\n",
    "    \n",
    "    rel_expanded['arg1_with_pos'] = arg1_final_with_pos_str\n",
    "    rel_expanded['arg2_with_pos'] = arg2_final_with_pos_str\n",
    "    rel_expanded['rel_with_pos'] = rel_final_with_pos_str\n",
    "    \n",
    "    return rel_expanded\n",
    "    \n",
    "    \n",
    "def create_node_attributes(n, annotation):\n",
    "    '''\n",
    "    This function takes a node (node_id) and returns its attributes \n",
    "    '''\n",
    "    if n is None:\n",
    "        return None\n",
    "    # ROOT does not appear in the tree\n",
    "    n_att = {}\n",
    "    if n == \"ROOT-NNP-0\":\n",
    "        n_word = \"ROOT\"\n",
    "        n_att[\"word\"] = n_word\n",
    "        n_att[\"id\"] = \"ROOT-NNP-0\"\n",
    "        return n_att\n",
    "    try:\n",
    "        # extract attributes\n",
    "        n_word, n_pos, n_ind = n.split('-')[0], n.split('-')[1], n.split('-')[2]        \n",
    "        n_ind = int(n_ind) - 1 # make it 0 base - ROOT becomes \"-1\"\n",
    "    except:\n",
    "        print error_msg(error_type=\"tokenizer\")\n",
    "        return None\n",
    "    #n_pos = annotation['pos'][n_ind][1]\n",
    "    \n",
    "    n_att[\"word\"] = n_word\n",
    "    n_att[\"ind\"] = n_ind\n",
    "    n_att[\"pos\"] = n_pos\n",
    "    n_att[\"id\"] = n\n",
    "    \n",
    "    return n_att\n",
    "\n",
    "def dp_str_to_node_id(w_ind_str,pos):\n",
    "    if w_ind_str == \"ROOT-0\":\n",
    "        return \"ROOT-NNP-0\"\n",
    "    word = w_ind_str.split('-')[0]\n",
    "    try:\n",
    "        word_ind = int(w_ind_str.split('-')[1])-1\n",
    "        res = word+\"-\" + pos[word_ind][1] + \"-\" + str(word_ind+1)\n",
    "    except:\n",
    "        print error_msg(error_type=\"tokenizer\")\n",
    "        return\n",
    "    return res\n",
    "    \n",
    "\n",
    "def create_dep_graph(annotation):\n",
    "    dep_parse = annotation['dep_parse']\n",
    "    if dep_parse == '':\n",
    "        return None\n",
    "    dp_list = dep_parse.split('\\n')\n",
    "    #print dp_list\n",
    "    pattern = re.compile(r'.+?\\((.+?), (.+?)\\)')    \n",
    "    #g = nx.Graph()\n",
    "    g_dir = nx.DiGraph()\n",
    "    for dep in dp_list:\n",
    "        m = pattern.search(dep)\n",
    "        n1 = dp_str_to_node_id(m.group(1),annotation['pos'])\n",
    "        n2 = dp_str_to_node_id(m.group(2),annotation['pos'])\n",
    "        n1_att = create_node_attributes(n1, annotation)\n",
    "        n2_att = create_node_attributes(n2, annotation)\n",
    "        if n1_att is None or n2_att is None:\n",
    "            return None\n",
    "        \n",
    "        g_dir.add_node(n1, n1_att)\n",
    "        g_dir.add_node(n2, n2_att)\n",
    "        e_rel = dep[:dep.find(\"(\")]\n",
    "        #edges.append(e)\n",
    "        g_dir.add_edge(n1, n2, {'rel' : e_rel}, label = e_rel)\n",
    "    return g_dir\n",
    "\n",
    "def get_simp_rel(rel, option = \"SVO\", dataset=DATA_SET):\n",
    "    # add options later\n",
    "    '''\n",
    "    Lower case, Strip\n",
    "    '''\n",
    "    arg1 = word_id_get_word(rel['arg1']).lower().strip()\n",
    "    arg2 = word_id_get_word(rel['arg2']).lower().strip()\n",
    "    r = word_id_get_word(rel['rel']).lower().strip()\n",
    "\n",
    "    '''\n",
    "    Mapping:\n",
    "    (I,You,We -> Parents)\n",
    "    '''    \n",
    "    if dataset == \"Mothering\":\n",
    "        parent_list = [\"i\",\"you\",\"we\",\"us\"]\n",
    "        if arg1 in parent_list:\n",
    "            arg1 = \"parent\"\n",
    "        if arg2 in parent_list:\n",
    "            arg2 = \"parent\"\n",
    "\n",
    "        child_list = [\"child\",\"children\",\"kid\",\"kids\",\"son\", \"sons\",\"daughter\",\"daughters\",\"toddler\",\"toddlres\",\"boy\"]\n",
    "        if arg1 in child_list:\n",
    "            arg1 = \"child\"\n",
    "        if arg2 in child_list:\n",
    "            arg2 = \"child\"    \n",
    "    '''\n",
    "    Stemming\n",
    "    '''\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    arg1 = stemmer.stem(arg1) \n",
    "    arg2 = stemmer.stem(arg2)\n",
    "    r = stemmer.stem(r)\n",
    "    \n",
    "    rel_simp = rel.copy()\n",
    "    rel_simp['arg1'] = arg1\n",
    "    rel_simp['arg2'] = arg2\n",
    "    rel_simp['rel'] = r\n",
    "    return rel_simp\n",
    "\n",
    "\n",
    "\n",
    "def get_relations(g_dir, annotation, option=\"SVO\"):\n",
    "    relations = []\n",
    "    '''\n",
    "    Simplified relations:\n",
    "    meaning that we only keep head words, do stemming, map words to their actual actor ( I,we,you -> parents)\n",
    "    '''\n",
    "    relations_simp = [] \n",
    "    if option == \"SVO\":\n",
    "        t_verbs = annotation['verbs']\n",
    "        for v in t_verbs:\n",
    "            v_id = word_to_node_id(v,annotation)\n",
    "            try:\n",
    "                g_dir_v = g_dir[v_id] #adjacency of v_id\n",
    "            except:\n",
    "                print v_id, \" does not appeared as a separate node in parsing tree.\"\n",
    "                continue\n",
    "            nsubj_list = []\n",
    "            dobj_list = []\n",
    "            for word_id, e in g_dir_v.iteritems():\n",
    "                if e[\"rel\"] == \"nsubj\":\n",
    "                    nsubj_list.append(word_id)\n",
    "                if e[\"rel\"] == \"dobj\":\n",
    "                    dobj_list.append(word_id)\n",
    "            if len(nsubj_list) > 0 and len(dobj_list) > 0:\n",
    "                for s in nsubj_list:\n",
    "                    for o in dobj_list:\n",
    "                        rel = {}\n",
    "                        rel[\"rel\"] = v_id\n",
    "                        rel[\"arg1\"] = s#s.split(\"-\")[0]\n",
    "                        rel[\"arg2\"] = o#o.split(\"-\")[0]\n",
    "                        rel[\"type\"] = option\n",
    "                        rel[\"pattern\"] = \"(nsubj, verb, dobj)\"\n",
    "                        rel_expanded = expand_rel(rel, g_dir, annotation)\n",
    "                        relations.append(rel_expanded.copy())\n",
    "                        rel_simp = get_simp_rel(rel_expanded.copy(),option)\n",
    "                        relations_simp.append(rel_simp)\n",
    "    return relations, relations_simp\n",
    "\n",
    "def create_argument_graph(df, source, target, edge_attr=None, graph_type=\"directed\"):\n",
    "    ''' Return a graph from Pandas DataFrame.\n",
    "    Modified version of \"from_pandas_dataframe\" function.\n",
    "    '''\n",
    "    if graph_type == \"undirected\":\n",
    "        g = nx.Graph()\n",
    "    elif graph_type == \"directed\":\n",
    "        g = nx.DiGraph()\n",
    "    else:\n",
    "        g = nx.MultiGraph()\n",
    "    \n",
    "    src_i = df.columns.get_loc(source)\n",
    "    tar_i = df.columns.get_loc(target)\n",
    "    label_i = df.columns.get_loc(edge_attr)\n",
    "    if edge_attr:\n",
    "        # If all additional columns requested, build up a list of tuples\n",
    "        # [(name, index),...]\n",
    "        if edge_attr is True:\n",
    "            # Create a list of all columns indices, ignore nodes\n",
    "            edge_i = []\n",
    "            for i, col in enumerate(df.columns):\n",
    "                if col is not source and col is not target:\n",
    "                    edge_i.append((col, i))\n",
    "        # If a list or tuple of name is requested\n",
    "        elif isinstance(edge_attr, (list, tuple)):\n",
    "            edge_i = [(i, df.columns.get_loc(i)) for i in edge_attr]\n",
    "        # If a string or int is passed\n",
    "        else:\n",
    "            edge_i = [(edge_attr, df.columns.get_loc(edge_attr)),]\n",
    "\n",
    "        # Iteration on values returns the rows as Numpy arrays\n",
    "        for row in df.values:\n",
    "            g.add_edge(row[src_i], row[tar_i], label = row[label_i])#{i:row[j] for i, j in edge_i},label=row[label_i])\n",
    "    \n",
    "    # If no column names are given, then just return the edges.\n",
    "    else:\n",
    "        for row in df.values:\n",
    "            g.add_edge(row[src_i], row[tar_i])\n",
    "\n",
    "    return g\n",
    "\n",
    "def create_argument_multiGraph(df, source, target,edge_attr):\n",
    "\n",
    "    src_i = df.columns.get_loc(source)\n",
    "    tar_i = df.columns.get_loc(target)\n",
    "    label_i = df.columns.get_loc(edge_attr)\n",
    "    \n",
    "    g = nx.MultiDiGraph()\n",
    "    nodes = set()\n",
    "    nodes = list(nodes.union(df[source],df[target]))\n",
    "    for n in nodes:\n",
    "        g.add_node(n)\n",
    "        ''' Get dataframe in which n is the source'''\n",
    "        df_n = df[df[source] == n]\n",
    "        cnt = Counter()\n",
    "        for row in df_n.values:\n",
    "            cnt[(row[label_i],row[tar_i])] += 1\n",
    "        for k,v in cnt.most_common():\n",
    "            #print n,k,v\n",
    "            label_rel_freq = str(k[0])+\"-\"+str(v)\n",
    "            g.add_edge(n,str(k[1]),label=label_rel_freq)\n",
    "    return g\n",
    "\n",
    "def filter_nodes(df,source,target, selected_nodes):\n",
    "    df_filtered = df[np.logical_and(df[source].isin(selected_nodes), df[target].isin(selected_nodes))]\n",
    "    return df_filtered\n",
    "\n",
    "def glob_version(entity, entity_versions):\n",
    "    '''\n",
    "    Extraction part -> arg or rel\n",
    "    Take an argument or relation entry, with a list of the main actors and different versions of the main actors.\n",
    "    Return the global name for the main actor.\n",
    "    '''\n",
    "    entity_new = \"\"\n",
    "    entity_new = entity\n",
    "    entity_new = entity_new.lower()\n",
    "    entity_head = re.search(r'\\{(.*)\\}', entity_new).group(1)\n",
    "    for ent_glob_name, ent_version_list in entity_versions.iteritems():\n",
    "        if entity_head in ent_version_list:\n",
    "            entity_new = ent_glob_name\n",
    "            break\n",
    "    return entity_new\n",
    "    \n",
    "def get_simp_df(df,entity_versions):\n",
    "    for index, row in df.iterrows():\n",
    "        # lower case the letters\n",
    "        arg1_new = glob_version(row['arg1'],entity_versions)\n",
    "        arg2_new = glob_version(row['arg2'],entity_versions)\n",
    "        row['arg1'] = arg1_new\n",
    "        row['arg2'] = arg2_new\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
